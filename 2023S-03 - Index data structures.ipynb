{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cd5f3b-91d1-49dd-90d6-3c5f9d4337a8",
   "metadata": {},
   "source": [
    "# Where is a hospital in Manhattan Downtown?\n",
    "\n",
    "In this lab we will create 2 vector indices to answer a very simple question: if you are in Manhattan downtown, where is the nearest hospital? We will base our soultion on two sources of data:\n",
    "- [Points of Interest dataset](https://drive.google.com/file/d/1LUudtCADqSxRl18ZzCzyPPGfhuUo2ZZs/view?usp=sharing). This is a 10% sample of a bigger dataset. Download and uncompress the file.\n",
    "- [Google Geocoding API](https://developers.google.com/maps/documentation/geocoding/start) or any other [equivalent service](https://gisgeography.com/geocoders/). For Google you will need to obtain a key. **PLEASE DO NOT SUBMIT THE KEY TO MOODLE :)**\n",
    "\n",
    "\n",
    "# 0. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5311c-46bd-459e-9aaa-68ac3f1a088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def draw_earth(xlim=(-180, +180), ylim=(-90, +90)):\n",
    "    import matplotlib.pyplot\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    \n",
    "    # this file also lives in github. Adjust the path if needed.\n",
    "    df = pd.read_csv(\"world.csv\")\n",
    "    \n",
    "    for row in df['geojson']:\n",
    "        js = json.loads(row)\n",
    "        polys = js['coordinates']\n",
    "        for poly in polys:\n",
    "            for pp in poly:\n",
    "                x, y = [v[0] for v in pp], [v[1] for v in pp]\n",
    "                plt.plot(x, y, color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095e09a-bde2-4854-9b4c-774980032d46",
   "metadata": {},
   "source": [
    "Reading the dataset and storing coordinates in `GEO` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a70144-746d-443d-9a94-2ea16f7d2209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# replace filename if you want to use another data file\n",
    "# be careful! 2M points is still a big number and can eat significant amout of memory\n",
    "with open(\"poi_sample01.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "# let's sample 20000 of points to draw\n",
    "step = len(dataset) // 20000\n",
    "\n",
    "# pure coordinated in compressed representation, 2B per number -> 8MB per array\n",
    "#GEO = np.array([v[0] for v in dataset], dtype=np.float16)\n",
    "GEO = np.array([v[0] for v in dataset], dtype=np.float16) + np.random.normal(0, 0.01, np.array([v[0] for v in dataset], dtype=np.float16).shape)\n",
    "N = len(dataset)\n",
    "# free the memory\n",
    "dataset = None\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b4471-6023-4baa-9922-6427371ce00d",
   "metadata": {},
   "source": [
    "Showing approximate dataset data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34accde-8889-4c48-8979-90801bc33b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_earth(ylim=(-75, 75))\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.grid()\n",
    "plt.scatter(GEO[::step, 0], GEO[::step, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df552af0-04b6-486a-9e76-b19bfe957d46",
   "metadata": {},
   "source": [
    "## 0.1. [1 point] Ok. Let's prepare ourselves to read the data from the hard drive\n",
    "\n",
    "We will prepare id-based shards (data will be distributed into equal files with ranges `[0..capacity-1], [capacity..2*capacity-1], ...`. Each shard will store `capacity` elements. Your task is to complete the implementation with `iterate_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186d0a3-2783-4d0c-889a-a9760461bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_shards(file, folder='shard', capacity=20000):\n",
    "    import pickle, os, math, gc\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "    with open(file, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "    nshards = len(dataset) // capacity\n",
    "    if nshards * capacity < len(dataset):\n",
    "        nshards += 1\n",
    "    \n",
    "    for i in range(nshards):\n",
    "        with open(f\"{folder}/{i}\", 'wb') as f:\n",
    "            part = dataset[i * capacity:(i+1)*capacity]\n",
    "            pickle.dump(part, f)\n",
    "    dataset = None\n",
    "    gc.collect()            \n",
    "\n",
    "    \n",
    "def dataset_get(indices, folder='shard', capacity=20000) -> list:\n",
    "    result = []\n",
    "    groups = {}\n",
    "    for i in indices:\n",
    "        x = i // capacity\n",
    "        if x not in groups:\n",
    "            groups[x] = []\n",
    "        groups[x].append(i)\n",
    "    for x in groups:\n",
    "        with open(f\"{folder}/{x}\", \"rb\") as f:\n",
    "            sha = pickle.load(f)\n",
    "            for i in groups[x]:\n",
    "                row = sha[i % capacity]\n",
    "                result.append(row)\n",
    "    return result\n",
    "\n",
    "\n",
    "# should return iterator, which goes through all elements, consequently opening files\n",
    "# use ``yield`` operator to simplify your code\n",
    "def iterate_dataset(items, folder=\"shard\", capacity=20000):\n",
    "    nshards = items // capacity\n",
    "    if nshards * capacity < items:\n",
    "        nshards += 1\n",
    "    for n in range(nshards):\n",
    "        with open(f\"{folder}/{n}\", \"rb\") as f:\n",
    "            sha = pickle.load(f)\n",
    "            for elem in sha:\n",
    "                yield (elem, \"stub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2b7c1-2a37-48bc-9a59-f8aa2a12fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_shards(\"poi_sample01.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c72c8-ec98-4d69-a04c-a921bcdc9f3d",
   "metadata": {},
   "source": [
    "asserts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc03748-bb20-4abf-8ded-29a7691c7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for r in iterate_dataset(N):\n",
    "    i += 1\n",
    "\n",
    "assert i == N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfe74d-7423-415a-b94c-3b132954c9b6",
   "metadata": {},
   "source": [
    "asserts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8e8ee-ee4e-4e51-b1e8-0d6dc55c71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in [137, 40000, 600000]:\n",
    "    assert np.allclose(GEO[i,:], dataset_get([i])[0][0], atol=5*1e-2), \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397c665-55fd-4fbe-9557-c2fb13339f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_get([1, 10, 1000234, N-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae824d0-ed81-4fac-98fb-ff92e80dc752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# And now the task!\n",
    "Ok. We are ready to perform the following steps utilizing functions beyond:\n",
    "1. Build **coordinate search index**. We will use it to obtain POI from the given region.\n",
    "3. Implement **vector text embedding index** (Annoy, HNSW) to serve semantic queries.\n",
    "3. Implement **geocoding** with cache. We will use it to obtain city coordinates.\n",
    "4. Impement search for **double queries: town and location type**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac21fa4-dc16-41b9-8c4f-7ea535b0dd75",
   "metadata": {},
   "source": [
    "# 1. Create spacial index for points of interest\n",
    "\n",
    "We will store dataset rows numbers as values, and coordinates as keys. Please use [KDtree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) or [BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) from sklearn.\n",
    "\n",
    "## 1.1. [5 points] Build the index and return it\n",
    "\n",
    "Implement the following functions. To get the full grade:\n",
    "- `build_geospacial_index` should build and return a search tree object: KDTree or BallTree.\n",
    "- `kNN` accepts a 2D-point, `k` neighbours parameter, and returns **approximate** `k` neighbours (they can be different from the real neighbours).\n",
    "- `inRadius` accepts a 2D-point, L<sub>2</sub> `radius`, and returns points inside the radius. Clarification: for simplicity **radius is given in units of coordinates (degrees)**, not kilometers or meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e216ac-fd46-475e-8082-158ba32c4661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "def build_geospacial_index(points, leaf_size=5) -> BallTree: # or KDTree here\n",
    "    tree = BallTree(points, leaf_size)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def kNN(query_point: list, k: int, index: BallTree) -> list:\n",
    "    _, ind = index.query([query_point], k=k)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def inRadius(query_point: list, r: float, index: BallTree) -> list:\n",
    "    ind = index.query_radius([query_point], r=r)\n",
    "    return ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(len(GEO)):\n",
    "    coorx, coory = np.random.normal(0.0, 0.01,2)\n",
    "    if i%10000 == 0:\n",
    "        print(GEO[i], [GEO[i][0] + coorx, GEO[i][1] + coory])\n",
    "    GEO[i] = [GEO[i][0] + coorx, GEO[i][1] + coory]\n",
    "\n",
    "print(time.time() - start)\n",
    "#print(random.uniform(0.0, 0.01))\n",
    "print(np.random.normal(0.0, 0.01,2))\n",
    "print(np.random.normal(0.0, 0.01))\n",
    "print(np.random.normal(0.0, 0.01))\n",
    "print(np.random.normal(0.0, 0.01))\n",
    "#GEO = np.array([v[0] for v in dataset], dtype=np.float16) + np.random.normal(0, 0.01, np.array([v[0] for v in dataset], dtype=np.float16).shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82f9fe-efdd-48ca-a859-a91339828693",
   "metadata": {},
   "outputs": [],
   "source": [
    "spaidx = build_geospacial_index(GEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff3d8c-81a5-4e4d-9b26-d76b67a98a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 13\n",
    "\n",
    "idx = kNN(GEO[test_id], 10, spaidx)\n",
    "print(sorted(idx))\n",
    "assert test_id in idx, \"Point itself should be in results\"\n",
    "\n",
    "idx = inRadius(GEO[test_id], 0.0625, spaidx)\n",
    "print(sorted(idx))\n",
    "assert test_id in idx, \"Point itself should be in results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae0ac1-d762-4dcc-bde5-1a67e84f884b",
   "metadata": {},
   "source": [
    "## 1.2. [4 points] Tricky assert\n",
    "\n",
    "Some keys (coordinates) in the dataset (surprise!) are duplicates. Unfortunately search trees (in basic implemenation) cannot support duplicates. Thus you can follow one of the strategies:\n",
    "- a key (coordinateS) corresponds to multiple values. This may require additional data strictures.\n",
    "- improve the data (coordinates) to avoid collisions (e.g. make sure they never coinside by adding insignificant noise)\n",
    "\n",
    "Pass the assert to get full points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933c8b6-0904-4724-a349-9e1375610553",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [1966663, 1480877, 2126566]\n",
    "for p in points:\n",
    "    x = GEO[p, :]\n",
    "    r = kNN(x, 1000, spaidx)\n",
    "    assert (p in r), \"Query did not return itself\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85991d1-b730-4405-a33f-8812e562145a",
   "metadata": {},
   "source": [
    "## 1.3. [Optional demo] How leaf size influences build and search speed?\n",
    "\n",
    "Let us look at how parameter of leaf size affects speed of search and construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc09c3b-cb03-44ea-8c15-ec3787fa090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "queries = random.sample(range(N), 1000)\n",
    "leaf_sizes = [1, 10, 20, 50]\n",
    "\n",
    "build_times = []\n",
    "query_times = []\n",
    "for ls in tqdm.tqdm(leaf_sizes):\n",
    "    start = time.time()\n",
    "    idx = build_geospacial_index(GEO, ls)\n",
    "    build_times.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    for q in queries:\n",
    "        d, r = spaidx.query([GEO[q]], 10000, sort_results=False, breadth_first=True)\n",
    "    query_times.append(time.time() - start)\n",
    "    idx = None\n",
    "    gc.collect()\n",
    "\n",
    "plt.xlabel(\"build time, s\")\n",
    "plt.ylabel(\"query time, s\")\n",
    "plt.scatter(build_times, query_times)\n",
    "for i, ls in enumerate(leaf_sizes):\n",
    "    plt.annotate(str(ls), (build_times[i], query_times[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a1830-2bd2-4e29-898e-300f43ca6dfc",
   "metadata": {},
   "source": [
    "## 1.4. [5 points] Range queries?\n",
    "\n",
    "Ok, you have a **radius query**, but what about **rectangual ranges**? Using the functions you already wrote, please, implement the range query given `north-east` and `south-west` corners. Pass the asserts to get points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518149b1-21e3-4f8c-93a9-4d955f7c9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# should return ids of the rows in this range\n",
    "def get_in_range(ne, sw, spacial_index, GEO) -> list:\n",
    "    r = np.sqrt((ne[0]-sw[0])**2 + (sw[1]-ne[1])**2)/2\n",
    "    centerx = sw[0] + (np.abs(ne[0]- sw[0])) / 2\n",
    "    centery =  sw[1] + (np.abs(sw[1]-ne[1])) / 2\n",
    "    idx = inRadius([centerx,centery], r, spacial_index)\n",
    "    rect_range = []\n",
    "    for elem in idx:\n",
    "        if sw[0]<= GEO[elem][0] <= ne[0] and sw[1] <= GEO[elem][1] <= ne[1]:\n",
    "            rect_range.append(elem)\n",
    "\n",
    "    return rect_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ids = get_in_range([-73.97, 40.75], [-74.03, 40.70], spaidx, GEO)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4c18d-e4d1-4fa5-a947-a61ff2c007c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_starbucks(ids):\n",
    "    for row in dataset_get(ids):\n",
    "        if 'Starbucks' in row[1]:\n",
    "            print(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d95a52-1667-4381-acce-686f99e11a8c",
   "metadata": {},
   "source": [
    "asserts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699927cb-ac44-498a-9dec-1d76a34df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_in_range([-73.97, 40.75], [-74.03, 40.70], spaidx, GEO)\n",
    "print(ids)\n",
    "assert any(map(\n",
    "            lambda x: 'Manhattan, 80 Delancey St' in x[1], \n",
    "            dataset_get(ids))), \"This Starbucks should be in place!\"\n",
    "\n",
    "print_starbucks(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17837e0b-7018-4e58-9624-19d0097a46c7",
   "metadata": {},
   "source": [
    "# 2. Geocoding\n",
    "\n",
    "In this block we will learn, how to convert text place names into coordinate rectangles.\n",
    "\n",
    "## 2.1. [5 points] Implement geocoding\n",
    "which returns north-eastern and south-western points of the place. Pass asserts to get full points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cee150-55a9-4501-a31c-fd7a4796d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# this function returns a pair of tuples: NE and SW corners.\n",
    "def get_town_range_coordinates(town: str, google_maps_api_key: str) -> tuple:\n",
    "    api = f\"http://api.positionstack.com/v1/forward?access_key={google_maps_api_key}&query={town}&bbox_module=1\"\n",
    "    text = requests.get(api).text\n",
    "    data = json.loads(text)[\"data\"][0]\n",
    "    location = data[\"bbox_module\"]\n",
    "    return [location[2], location[3]], [location[0], location[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6da1e0-a57e-47d5-acd7-3d0ed2b5d60f",
   "metadata": {},
   "source": [
    "If needed, request your key here: https://developers.google.com/maps/documentation/geocoding/get-api-key\n",
    "\n",
    "Note, that from the **local machine** you may (!) need to have VPN enabled. For **colab** this should work smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9c42a-6057-4408-a79f-689298dff17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_google_maps_api_key = open('google.key', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848f5bb-9acd-44a2-ba25-dae0d847f1cb",
   "metadata": {},
   "source": [
    "asserts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42826b-91e0-470f-8f9e-c8986894c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = get_town_range_coordinates('Pittsburgh downtown', my_google_maps_api_key)\n",
    "#print(p)\n",
    "assert p[1][0] <= -80. <= p[0][0] and p[1][1] <= 40.44 <= p[0][1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a8de0-7528-490a-855c-1cd22497accb",
   "metadata": {},
   "source": [
    "## 2.2. [5 points] Town queries\n",
    "\n",
    "Now, having a range query and geocoding, we can implement town-queries! Pass the assert to get the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a7f60-5bd3-430b-86d1-43be9a12345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should return dataset indices\n",
    "def get_in_town(town, index, GEO, maps_api_key) -> list:\n",
    "    ne, sw = get_town_range_coordinates(town, maps_api_key)\n",
    "    idx = get_in_range(ne,sw, index, GEO)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b1f42-f5e7-45a0-a939-c8b7daef7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_in_town('Pittsburgh downtown', spaidx, GEO, my_google_maps_api_key)\n",
    "\n",
    "assert any(map(\n",
    "            lambda x: 'US, Pittsburgh, 810 River Ave' in x[1], \n",
    "            dataset_get(ids))), \"This Starbucks should be in place!\"\n",
    "\n",
    "print_starbucks(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c1670-3314-4b81-95e8-278a5b8822db",
   "metadata": {},
   "source": [
    "## 2.3. [5 points] Caching\n",
    "\n",
    "Why should you pay for every geocaching request, if you can cache them? Implement a cached version on geocoding. The second query does not use internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GEO_CACHE = {}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3848261-07c1-47ef-bcea-1b0a72c70038",
   "metadata": {},
   "outputs": [],
   "source": [
    "global GEO_CACHE\n",
    "\n",
    "def get_town_range_coordinates_cached(town: str, maps_key: str) -> tuple:\n",
    "    global GEO_CACHE\n",
    "    if town in GEO_CACHE.keys():\n",
    "        print(\"Cache Hit!\")\n",
    "        return GEO_CACHE[town]\n",
    "    idx = get_town_range_coordinates(town, maps_key)\n",
    "    GEO_CACHE[town] = idx\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_in_town_cached(town: str, index, GEO, maps_key: str) -> list:\n",
    "    if town in GEO_CACHE.keys():\n",
    "        print(\"Cache Hit!\")\n",
    "        return GEO_CACHE[town]\n",
    "    idx = get_in_town(town, index, GEO, maps_key)\n",
    "    GEO_CACHE[town] = idx\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf7790-ba8f-472a-9c67-133ffa8bc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_in_town_cached('Boulder, CO', spaidx, GEO, my_google_maps_api_key)\n",
    "print_starbucks(ids)\n",
    "ids = get_in_town_cached('Boulder, CO', spaidx, GEO, my_google_maps_api_key)\n",
    "print_starbucks(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7cb00-c98a-493b-a3bd-f290fcfc83a4",
   "metadata": {},
   "source": [
    "# 3. Text search\n",
    "\n",
    "We are done with geography, but we have no clear method to search for categories. What if we prepare vector index of location names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5e36e-3cc0-4b0e-9cc5-be1ffec44768",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea12030-de9c-4200-a411-0138e43c05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dca13a-5641-4330-b1e2-895f6a81e220",
   "metadata": {},
   "source": [
    "## 3.1. [5 points] Embedding\n",
    "\n",
    "Here is the trick. If you use any embedding model \"as it is\", it may take some hours to prepare 2M embeddings. It's ok if you can wait (and get **2 points**), but...\n",
    "\n",
    "To get full points, please think, how you can speed up the process with embedding to less than 5 minutes?\n",
    "\n",
    "HINT: spacy model `nlp` has a [dictionary for word embeddings](https://spacy.io/api/vocab). You can access `nlp.vocab[word].vector` to get word embedding, `nlp.vocab.strings` map stores integer indices. Or maybe you want to use `fasttext` instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd2b67c2-63f0-444d-9720-83f6c3b98d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# if you need most frequent english words for some reason.\n",
    "WORDS = set([a.strip() for a in open('words.txt', 'r').readlines()])\n",
    "\n",
    "def embed(text, nlp):\n",
    "    # TODO your code here. This should return a vector (300,)\n",
    "    arr = []\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        nlp.vocab[w]\n",
    "        if w in nlp.vocab:\n",
    "            arr.append(nlp.vocab[w].vector)\n",
    "    sem = np.mean(arr, axis=0)\n",
    "    #print(sem)\n",
    "    return sem / np.linalg.norm(sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43836204-638f-4f77-b4b2-7348fb0241dd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "embeddings = np.zeros((N, 300), dtype=np.float16)\n",
    "\n",
    "for i, item in enumerate(tqdm(iterate_dataset(N), total=N)):\n",
    "    name = item[1].split('.')[0]\n",
    "    emb = embed(name, nlp)\n",
    "    if emb is not None:\n",
    "        embeddings[i, :] = emb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73177781-3e59-494b-afc6-215f1a926f24",
   "metadata": {},
   "source": [
    "## 3.2. [10 points] Vector index\n",
    "\n",
    "Here you build vector index for our embeddings. I want to warn Windows users, that they can observe problems with installing Faiss and HNSWlib (please refer to the corresponding lab). Still this is not the reason not to try :)\n",
    "Choose **one of the libraries** and fulfill the requirements to get full points:\n",
    "1. If you choose [FAISS](https://faiss.ai/). Get started with [installation](https://faiss.ai/#install) and this [tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started). To get full points your index must use [Product Quantization](https://github.com/facebookresearch/faiss/wiki/Lower-memory-footprint): 50 subvectors, 8 bits (1 byte) each. Use custom `nprobe` parameter equal to 23. Number or Voronoi cells is `65536`. Refer to [this document](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index#if-1m---10m-ivf65536_hnsw32) to understand recommendations.\n",
    "2. If you use [HNSWlib](https://github.com/nmslib/hnswlib) (or [nmslib](https://github.com/nmslib/nmslib)) then follow these requirements. Use `cosine` metric for index construction, maximum number of outgoing connections (max outdegree) in the graph is 16, `ef` parameter at construction time should be `250`. Some useful information is given [here](https://github.com/nmslib/nmslib/blob/master/manual/methods.md).\n",
    "3. For [Annoy](https://github.com/spotify/annoy) you should use cosine distance for the space (if vectors are normed, you can use dot product intead), use all CPU cores at construction time. Build the index right on the disk, then load. Your index should consist of 37 trees.\n",
    "\n",
    "**NB** If you run on not-very-modern hardware (e.g. your RAM is less then 8GB), then you'd better reduce dataset size (e.g. take a specific region only like US east cost). You can also reduce other parameters only for the sake of RAM efficiency, but please specify and justify your decisions.\n",
    "\n",
    "e.g.\n",
    "```    \n",
    "roi = set(get_in_range([-68.645945, 43.163175], [-80.461502, 37.097044], spaidx, GEO))\n",
    "```\n",
    "\n",
    "\n",
    "**HINT** You can remove `embeddings` array and call `gc.collect()` before loading index to RAM.\n",
    "\n",
    "**HINT2** Should you index vectors with $[0]^N$ values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d07f72e7-533b-43b7-beb1-dd50f4580ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b1a228f-4b29-4bee-af78-79fa0df6eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hnswlib\n",
    "def get_vector_index():\n",
    "    # TODO returns index object\n",
    "    ind = hnswlib.Index(\"cosine\", 300)\n",
    "    ind.init_index(N, 16, 250)\n",
    "    ind.add_items(embeddings)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def get_kNN_embeddings(embedding, k, index):\n",
    "    # TODO write your kNN queries here\n",
    "    labels, distances = index.knn_query(embedding, k)\n",
    "    return distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a4c7c11-1fee-42b2-b8f8-c600928bd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004.9357216358185\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "embedding_index = get_vector_index()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02176  0.1177   0.00952 ... -0.03397 -0.02159  0.03308]\n",
      " [-0.02176  0.1177   0.00952 ... -0.03397 -0.02159  0.03308]\n",
      " [-0.02176  0.1177   0.00952 ... -0.03397 -0.02159  0.03308]\n",
      " ...\n",
      " [-0.02176  0.1177   0.00952 ... -0.03397 -0.02159  0.03308]\n",
      " [-0.02176  0.1177   0.00952 ... -0.03397 -0.02159  0.03308]\n",
      " [-0.02176  0.1177   0.00952 ... -0.03397 -0.02159  0.03308]]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(embed('pharmacy', nlp)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5600f9a9-2713-4e43-832c-55dc4b4219be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = get_kNN_embeddings(embed('pharmacy', nlp), 1000, embedding_index)\n",
    "\n",
    "assert len(result) == 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e690d-a6f8-4c9a-8a89-eb02aabbc690",
   "metadata": {},
   "source": [
    "# 4. [5 points] And now we want to have this together!\n",
    "\n",
    "Say no more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7dd5aab-f429-4d51-8691-2fd497c23d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(town, query) -> list:\n",
    "    data = get_in_town_cached(town, spaidx, GEO, my_google_maps_api_key)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ce6d2a8-afb3-4ebd-8669-5d49a977808b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[84], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m items \u001B[38;5;241m=\u001B[39m \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mManhattan downtown\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhospital\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(items[:\u001B[38;5;241m20\u001B[39m])\n\u001B[0;32m      3\u001B[0m xy \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([row[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m items])\n",
      "Cell \u001B[1;32mIn[83], line 2\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(town, query)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfind\u001B[39m(town, query) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mget_in_town_cached\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtown\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspaidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGEO\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmy_google_maps_api_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(data)\n\u001B[0;32m      4\u001B[0m     knn \u001B[38;5;241m=\u001B[39m get_kNN_embeddings(embed(query, nlp), \u001B[38;5;241m1000\u001B[39m, embedding_index)\n",
      "Cell \u001B[1;32mIn[29], line 17\u001B[0m, in \u001B[0;36mget_in_town_cached\u001B[1;34m(town, index, GEO, maps_key)\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCache Hit!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m GEO_CACHE[town]\n\u001B[1;32m---> 17\u001B[0m idx \u001B[38;5;241m=\u001B[39m \u001B[43mget_in_town\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtown\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGEO\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaps_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m GEO_CACHE[town] \u001B[38;5;241m=\u001B[39m idx\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m idx\n",
      "Cell \u001B[1;32mIn[26], line 3\u001B[0m, in \u001B[0;36mget_in_town\u001B[1;34m(town, index, GEO, maps_api_key)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_in_town\u001B[39m(town, index, GEO, maps_api_key) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m----> 3\u001B[0m     ne, sw \u001B[38;5;241m=\u001B[39m \u001B[43mget_town_range_coordinates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtown\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaps_api_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     idx \u001B[38;5;241m=\u001B[39m get_in_range(ne,sw, index, GEO)\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m idx\n",
      "Cell \u001B[1;32mIn[23], line 9\u001B[0m, in \u001B[0;36mget_town_range_coordinates\u001B[1;34m(town, google_maps_api_key)\u001B[0m\n\u001B[0;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(text)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      8\u001B[0m location \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox_module\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mlocation\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m, location[\u001B[38;5;241m3\u001B[39m]], [location[\u001B[38;5;241m0\u001B[39m], location[\u001B[38;5;241m1\u001B[39m]]\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "items = find('Manhattan downtown', 'hospital')\n",
    "print(items[:20])\n",
    "xy = np.array([row[0] for row in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b66ed-e15a-4003-809f-67f6bc585b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "NE, SW = get_town_range_coordinates_cached('Manhattan downtown', my_google_maps_api_key)\n",
    "draw_earth(xlim=(SW[0] - 5, NE[0] + 5), ylim=(SW[1] - 5, NE[1] + 5))\n",
    "plt.scatter(xy[:, 0], xy[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
